<html><head><meta charset="utf-8"><script src="https://cdn.jsdelivr.net/npm/chart.js@2.8.0"></script>
 <link rel="stylesheet"
       href="http://cdn.jsdelivr.net/gh/highlightjs/cdn-release@9.17.1/build/styles/default.min.css">
 <script src="http://cdn.jsdelivr.net/gh/highlightjs/cdn-release@9.17.1/build/highlight.min.js"></script>
 <script>hljs.initHighlightingOnLoad();</script>
 <link href="../css/all.css" rel="stylesheet">
<link rel="stylesheet" href="../mystyle.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/pretty-checkbox@3.0/dist/pretty-checkbox.min.css">
</head>
<body>
 <ul class="publist-inline" style="text-align:left;"><li class="web"><a href="../index.html">< Index</a></ul><h1 class="title">VisemeNet: Audio-Driven Animator-Centric Speech Animation <i class="fas fa-circle graphcol0" style="font-size:150%;color:#0868ac;"></i> <i class="fas fa-square graphcol6"  style="font-size:150%;color:#7570b3;"></i></h1><ul class="authors">
<li> Yang <span class="family">Zhou</span></li>
<li> Zhan <span class="family">Xu</span></li>
<li> Chris <span class="family">Landreth</span></li>
<li> Evangelos <span class="family">Kalogerakis</span></li>
<li> Subhransu <span class="family">Maji</span></li>
<li> Karan <span class="family">Singh</span></li>
</ul>
<center> SIGGRAPH 2018</center><ul class="publist-inline">
<li class="web"> <i class="fas fa-globe-americas"></i> <a href="https://doi.org/10.1145/3197517.3201292">ACM</a></li>
<li class="pdf"> <i class="far fa-file-pdf"></i> <a href="http://www.dgp.toronto.edu/~karan/papers/visemenetSIG18.pdf">preprint</a></li>
<li class="web"> <i class="fas fa-globe-americas"></i> <a href="https://people.umass.edu/~yangzhou/visemenet/">Project page</a></li>
<li class="web"> <i class="far fa-file-alt"></i> <a href="https://github.com/yzhou359/VisemeNet_tensorflow">Code</a></li>
<li class="web"> <i class="fas fa-university"></i> <a href="https://arxiv.org/abs/1805.09488">arXiv or openarchive initiative</a></li>
<li class="web"> <i class="fas fa-database"></i> <a href="10.1145-3197517.3201292-metadata.json">DOI Metadata</a></li>
</ul>
<center><img width="300" src="10.1145-3197517.3201292-thumb.png"></img></center>
    <hr>
    <div class="row">
    <div class="column2 chart-container" style="position: relative; height:40vh; width:30vw">
    <canvas width="300" height="250" id="myChart" class="chartjs-render-monitor"></canvas>
    </div>
    <div class="column2"><h2>Informations</h2>
    <ul><li><span class="family">Paper topic</span>: Animation</li>
<li><span class="family">Nature of the artefact</span>: Code</li>
<li><span class="family">Able to run a replicability test</span>: Yes</li>
<li><span class="family">Replicability score</span>: 4</li>
<li><span class="family">License</span>: unspecified</li>
<li><span class="family">Build mechanism</span>: Not applicable (python, Matlab..)</li>
<li><span class="family">Mandatory dependencies</span>:Open-source libraries, Closed source (e.g. commercial) software or libraries free for research purposes</li>
<li><span class="family">Documentation score</span> {0,1,2}: 1</li>
<li><span class="family">Google Scholar Citation</span> (19/01/2020):   13</li>
<li><span class="family">Reviewer</span>: 2</li>
</ul><h2>Comments</h2><pre>Note that the VisemeNet code has strong requirements on software and library versions. The code does run on Python 3.5 but not on Python 3.6.5. Also, Python 3.5 now comes with a default scipy 1.4.0 which is not suitable, though an older scipy 1.1.0 works.
I could test the prediction with the provided trained network on the single provided audio file, as well as the Maya script to use the results on a public face rig. This worked nicely.
I did not test the training as data are only accessible upon (non-anonymous) request, and training instructions and scripts are not provided (although a non-standalone train_visemenet.py file is present, it does not run on its own).

--alternative test on linux--
I failed to have the tensorflow package within anaconda to work with libcudnn.so.8.0.</pre>
    </div>
    </div>
        <script>

          var ctx = document.getElementById('myChart');
          var myChart = new Chart(ctx, {
              type: 'radar',
              data: {
    labels: ['Dependencies', 'Build / Configure', 'Easy to adapt', 'Can replicate paper results'],
                     datasets: [{
                      label: 'Build/Run Experience (the higher, the better,  {1..5}, 0=N/A )',
                      backgroundColor: 'rgba(54, 162, 235, 0.3)',
                      borderColor: 'rgb(54, 162, 235)',
                      data: [4,5,5,4]
                  }]
              },
              options: {
  scale: {
      ticks: {
          suggestedMin: 0,
          suggestedMax: 5,
          stepSize: 1
      }
  }
}
          });


        </script>
    <br><br>
<br><br><ul class="publist-inline" style="text-align:left;font-size:110%"><li > <i ></i> <a href="replicability.json">Download complete data for this entry</a></li></ul></code></pre></body></html>